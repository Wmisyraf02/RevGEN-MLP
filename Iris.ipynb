{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fe94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from RevGEN_MLP import RevGEN_MLP  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.ensemble import IsolationForest,RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99246b2a",
   "metadata": {},
   "source": [
    "# RevGEN-MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94adf08",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b18b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test= train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_encoded =encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Scale features \n",
    "ANN_scaler = StandardScaler()\n",
    "X_train_scaled = ANN_scaler.fit_transform(X_train)\n",
    "X_test_scaled = ANN_scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e17dbb",
   "metadata": {},
   "source": [
    "## Training RevGEN-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 1\n",
    "num_epochs = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize model\n",
    "model = RevGEN_MLP(\n",
    "    n_layers=num_layer,\n",
    "    x=X_train_scaled[0].reshape(-1, 1),\n",
    "    y_actual=y_train_encoded[0].reshape(-1, 1),\n",
    "    epochs=num_epochs,\n",
    "    loss_function=\"cross_entropy\"\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epoch = 0\n",
    "    correct_train = 0\n",
    "\n",
    "    # Shuffle training indices\n",
    "    indices = np.random.permutation(X_train_scaled.shape[0])\n",
    "\n",
    "    for i in indices:\n",
    "        x_sample = X_train_scaled[i].reshape(-1, 1)\n",
    "        y_sample = y_train_encoded[i].reshape(-1, 1)\n",
    "\n",
    "        # Train and accumulate loss\n",
    "        model.train(input=x_sample, target=y_sample)\n",
    "        loss_epoch += model.loss_fn(input=x_sample, target=y_sample)\n",
    "\n",
    "        # Predict and count correct predictions\n",
    "        pred = model.forward(x_sample)\n",
    "        if np.argmax(pred[:3]) == np.argmax(y_sample):\n",
    "            correct_train += 1\n",
    "\n",
    "    # Compute average training metrics\n",
    "    avg_train_loss = loss_epoch / X_train_scaled.shape[0]\n",
    "    train_accuracy = correct_train / X_train_scaled.shape[0]\n",
    "\n",
    "    # Evaluate on test set every 10 epochs\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        correct_test = 0\n",
    "        test_loss_epoch = 0\n",
    "\n",
    "        for i in range(X_test_scaled.shape[0]):\n",
    "            x_sample = X_test_scaled[i].reshape(-1, 1)\n",
    "            y_sample = y_test_encoded[i].reshape(-1, 1)\n",
    "\n",
    "            pred = model.forward(x_sample)\n",
    "            if np.argmax(pred[:3]) == np.argmax(y_sample):\n",
    "                correct_test += 1\n",
    "\n",
    "            test_loss_epoch += model.loss_fn(input=x_sample, target=y_sample)\n",
    "\n",
    "        avg_test_loss = test_loss_epoch / X_test_scaled.shape[0]\n",
    "        test_accuracy = correct_test / X_test_scaled.shape[0]\n",
    "\n",
    "        print(f\"Epoch {epoch:3d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy * 100:.2f}% | \"\n",
    "              f\"Test Loss: {avg_test_loss:.4f} | Test Acc: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(X_test_scaled.shape[0]):\n",
    "    x_sample = X_test_scaled[i].reshape(-1, 1)\n",
    "    y_sample = y_test_encoded[i].reshape(-1, 1)\n",
    "\n",
    "    pred = model.forward(x_sample)\n",
    "    pred_class = np.argmax(pred[0:3])\n",
    "    true_class = np.argmax(y_sample)\n",
    "\n",
    "    y_preds.append(pred_class)\n",
    "    y_true.append(true_class)\n",
    "\n",
    "    if pred_class == true_class:\n",
    "        correct += 1\n",
    "\n",
    "\n",
    "\n",
    "test_accuracy = correct / X_test_scaled.shape[0]\n",
    "precision = precision_score(y_true, y_preds, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true, y_preds, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_true, y_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Precision:     {precision * 100:.2f}%\")\n",
    "print(f\"Recall:        {recall * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b769e",
   "metadata": {},
   "source": [
    "## Invertibility Check\n",
    "Small reconstruction errors close to zero are expected due to floating-point precision limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = X_test_scaled[0].reshape(-1, 1)\n",
    "\n",
    "# Unscale \n",
    "x_sample_unscaled = ANN_scaler.inverse_transform(x_sample.reshape(1, -1))\n",
    "print(\"Original Sample:\", x_sample_unscaled)\n",
    "\n",
    "# Forward pass\n",
    "pred = model.forward(x_sample)\n",
    "print(\"\\nOutput (Classes):\", pred[0:3].ravel())\n",
    "print(\"Output (Latent Variable):\", pred[3:].ravel())\n",
    "# Reconstruct\n",
    "reconstructed_sample = model.reverse(pred)\n",
    "\n",
    "# Unscale reconstruction\n",
    "reconstructed_sample_unscaled = ANN_scaler.inverse_transform(\n",
    "    reconstructed_sample.reshape(1, -1)\n",
    ")\n",
    "print(\"\\nReconstructed Sample:\", reconstructed_sample_unscaled.ravel())\n",
    "\n",
    "mse_scaled = np.mean((x_sample - reconstructed_sample)**2)\n",
    "mse_unscaled = np.mean((x_sample_unscaled - reconstructed_sample_unscaled)**2)\n",
    "print(\"\\nMSE Error (Scaled Data): \", mse_scaled)\n",
    "print(\"MSE Error (Unscaled Data): \", mse_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9f5fb",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "detector = IsolationForest(random_state=42).fit(X)\n",
    "scores = detector.decision_function(X)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(scores, bins=50)\n",
    "threshold = np.percentile(scores, 5)\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='5th percentile threshold')\n",
    "\n",
    "# Add label\n",
    "plt.text(threshold, plt.ylim()[1]*0.9, '5% threshold', color='red', rotation=90, va='top', ha='right')\n",
    "\n",
    "plt.title(\"Isolation Forest Anomaly Scores - Iris\")\n",
    "plt.xlabel(\"Anomaly Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec409fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_epsilon(model,epsilon):\n",
    "    pred_classes = []\n",
    "    inputs = []\n",
    "    original_inputs = []\n",
    "    original_classes = []\n",
    "    original_prob = []\n",
    "    pred_probability = []\n",
    "\n",
    "    for test,truth in zip(X_test_scaled,y_test_encoded):\n",
    "        x_sample = test.reshape(-1, 1)\n",
    "        pred = model.forward(x_sample)\n",
    "\n",
    "        class_probs = pred[0:3].ravel()\n",
    "        max_prob = np.max(class_probs)\n",
    "        if np.argmax(pred[:3]) == np.argmax(truth):\n",
    "            if max_prob >= 0.8:\n",
    "                original_prob.append(max_prob)\n",
    "                pred_class = np.argmax(class_probs)\n",
    "                original_inputs.append(ANN_scaler.inverse_transform(x_sample.reshape(1, -1))[0])\n",
    "                original_classes.append(pred_class)\n",
    "                vector = pred.copy()\n",
    "                \n",
    "                vector[3] = vector[3] + epsilon\n",
    "                \n",
    "                new_input = model.reverse(input=vector)\n",
    "                new_pred = model.forward(new_input)\n",
    "                max_prob = np.max(new_pred[0:3])\n",
    "                pred_probability.append(max_prob)\n",
    "                new_pred_class = np.argmax(new_pred[0:3])\n",
    "\n",
    "                new_input_unscaled = ANN_scaler.inverse_transform(new_input.reshape(1, -1))[0]\n",
    "\n",
    "                pred_classes.append(new_pred_class)\n",
    "                inputs.append(new_input_unscaled)\n",
    "    data = {\n",
    "    \"sepal length (cm)\": [],\n",
    "    \"sepal width (cm)\": [],\n",
    "    \"petal length (cm)\": [],\n",
    "    \"petal width (cm)\": [],\n",
    "    \"Class\": [],\n",
    "    \"Probability\": []\n",
    "    }\n",
    "\n",
    "    for sample in inputs:\n",
    "        data[\"sepal length (cm)\"].append(sample[0])\n",
    "        data[\"sepal width (cm)\"].append(sample[1])\n",
    "        data[\"petal length (cm)\"].append(sample[2])\n",
    "        data[\"petal width (cm)\"].append(sample[3])\n",
    "\n",
    "    for cls in pred_classes:\n",
    "        data[\"Class\"].append(cls)\n",
    "\n",
    "    for prob in pred_probability:\n",
    "        data[\"Probability\"].append(prob*100)\n",
    "\n",
    "    generated_df = pd.DataFrame(data=data)\n",
    "    return generated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c4d9a",
   "metadata": {},
   "source": [
    "### Anomaly Score Threshold\n",
    "\n",
    "The following code decides the anomaly score threshold used to guide generation. The first threshold represents moderately anomalous data whereas the second threshold represents extremely anomalous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_1 = np.percentile(scores,5)\n",
    "threshold_2 = -0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponents = np.linspace(-8, -2 , 600)\n",
    "positive_epsilons = 10 ** exponents\n",
    "negative_epsilons = -positive_epsilons\n",
    "epsilons = np.sort(np.concatenate([negative_epsilons, positive_epsilons]))\n",
    "\n",
    "threshold = threshold_2 # Change threshold as needed\n",
    "\n",
    "results = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    generated_df = generate_data_with_epsilon(model, epsilon)\n",
    "    anomaly_score_results = []\n",
    "\n",
    "    for cls in generated_df[\"Class\"].unique():\n",
    "        subset = generated_df[generated_df[\"Class\"] == cls]\n",
    "        features = subset.drop(columns=[\"Class\", \"Probability\"]).to_numpy()\n",
    "        score = detector.decision_function(features)\n",
    "\n",
    "        anomaly_score_results.append({\n",
    "            \"Class\": cls,\n",
    "            \"Score\": score.mean()\n",
    "        })\n",
    "\n",
    "    scored_df = pd.DataFrame(anomaly_score_results)\n",
    "    mean_score_by_class = scored_df.set_index(\"Class\")[\"Score\"]\n",
    "    below_threshold = mean_score_by_class[mean_score_by_class < threshold].to_dict()\n",
    "\n",
    "    results.append({\n",
    "        \"epsilon\": epsilon,\n",
    "        \"mean_score\": mean_score_by_class.to_dict(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a971393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = []\n",
    "for r in results:\n",
    "    for cls in r[\"mean_score\"]:\n",
    "        if cls not in class_names:\n",
    "            class_names.append(cls)\n",
    "class_names.sort()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cls in class_names:\n",
    "    eps = []\n",
    "    overlaps = []\n",
    "    for r in results:\n",
    "        eps.append(r[\"epsilon\"])\n",
    "        overlaps.append(r[\"mean_score\"].get(cls))\n",
    "    plt.plot(eps, overlaps, marker='o', label=f'Class {cls}')\n",
    "\n",
    "\n",
    "# Threshold line\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')\n",
    "\n",
    "# Log scale on x-axis\n",
    "\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Mean Anomaly Score (Isolation Forest)\")\n",
    "\n",
    "plt.title(\"Mean Anomaly Score (Isolation Forest) vs. Epsilon (Alpha = 1)\")\n",
    "plt.legend()\n",
    "plt.xscale(\"symlog\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epsilons = {}\n",
    "best_eps_overall = None\n",
    "max_abs_eps = 0  # Track largest absolute epsilon\n",
    "\n",
    "for cls in class_names:\n",
    "    best_pos_eps = None\n",
    "    best_neg_eps = None\n",
    "    best_pos_diff = float('inf')\n",
    "    best_neg_diff = float('inf')\n",
    "\n",
    "    for r in results:\n",
    "        score = r[\"mean_score\"].get(cls)\n",
    "        eps = r[\"epsilon\"]\n",
    "\n",
    "        if score is not None and score < threshold:\n",
    "            diff = threshold - score\n",
    "\n",
    "            if eps > 0 and diff < best_pos_diff:\n",
    "                best_pos_diff = diff\n",
    "                best_pos_eps = eps\n",
    "\n",
    "            elif eps < 0 and diff < best_neg_diff:\n",
    "                best_neg_diff = diff\n",
    "                best_neg_eps = eps\n",
    "\n",
    "    best_epsilons[cls] = {\n",
    "        \"best_positive\": best_pos_eps,\n",
    "        \"best_negative\": best_neg_eps\n",
    "    }\n",
    "\n",
    "    # Update global best epsilon if this one is larger in magnitude\n",
    "    for eps in [best_pos_eps, best_neg_eps]:\n",
    "        if eps is not None and abs(eps) > max_abs_eps:\n",
    "            max_abs_eps = abs(eps)\n",
    "            best_eps_overall = eps\n",
    "\n",
    "# Display results\n",
    "for cls, eps_dict in best_epsilons.items():\n",
    "    print(f\"Class {cls}:\")\n",
    "    print(f\"  Best positive epsilon: {eps_dict['best_positive']}\")\n",
    "    print(f\"  Best negative epsilon: {eps_dict['best_negative']}\")\n",
    "\n",
    "print(f\"\\nBest overall epsilon across all classes: {best_eps_overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983d68b",
   "metadata": {},
   "source": [
    "## Generated Confidently Classified Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34059fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = max_abs_eps\n",
    "\n",
    "\n",
    "pred_classes = []\n",
    "inputs = []\n",
    "original_inputs = []\n",
    "original_classes = []\n",
    "original_prob = []\n",
    "pred_probability = []\n",
    "\n",
    "for test,truth in zip(X_test_scaled,y_test_encoded):\n",
    "    x_sample = test.reshape(-1, 1)\n",
    "    pred = model.forward(x_sample)\n",
    "\n",
    "    class_probs = pred[0:3].ravel()\n",
    "    max_prob = np.max(class_probs)\n",
    "    if np.argmax(pred[:3]) == np.argmax(truth):\n",
    "        if max_prob >= 0.8:\n",
    "            original_prob.append(max_prob)\n",
    "            pred_class = np.argmax(class_probs)\n",
    "            original_inputs.append(ANN_scaler.inverse_transform(x_sample.reshape(1, -1))[0])\n",
    "            original_classes.append(pred_class)\n",
    "\n",
    "            # Revese pass for when epsilon is substracted to latent variables\n",
    "            vector_neg_epsilon = pred.copy()\n",
    "            vector_neg_epsilon[3] = vector_neg_epsilon[3] - epsilon\n",
    "\n",
    "            new_input_neg = model.reverse(input=vector_neg_epsilon)\n",
    "            new_pred = model.forward(new_input_neg)\n",
    "            max_prob = np.max(new_pred[0:3])\n",
    "            pred_probability.append(max_prob)\n",
    "            new_pred_class_neg = np.argmax(new_pred[0:3])\n",
    "\n",
    "            # Revese pass for when epsilon is added to latent variables\n",
    "            vector_pos_epsilon = pred.copy()\n",
    "            vector_pos_epsilon[3] = vector_pos_epsilon[3] + epsilon\n",
    "\n",
    "            new_input_pos = model.reverse(input=vector_pos_epsilon)\n",
    "            new_pred = model.forward(new_input_pos)\n",
    "            max_prob_pos = np.max(new_pred[0:3])\n",
    "            pred_probability.append(max_prob_pos)\n",
    "            new_pred_class_pos = np.argmax(new_pred[0:3])\n",
    "\n",
    "            new_input_unscaled_pos = ANN_scaler.inverse_transform(new_input_pos.reshape(1, -1))[0]\n",
    "            new_input_unscaled_neg = ANN_scaler.inverse_transform(new_input_neg.reshape(1, -1))[0]\n",
    "\n",
    "            pred_classes.append(new_pred_class_pos)\n",
    "            inputs.append(new_input_unscaled_pos)\n",
    "\n",
    "            pred_classes.append(new_pred_class_neg)\n",
    "            inputs.append(new_input_unscaled_neg)\n",
    "\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"sepal length (cm)\": [],\n",
    "    \"sepal width (cm)\": [],\n",
    "    \"petal length (cm)\": [],\n",
    "    \"petal width (cm)\": [],\n",
    "    \"Class\": [],\n",
    "    \"Probability\": []\n",
    "}\n",
    "\n",
    "for sample in inputs:\n",
    "    data[\"sepal length (cm)\"].append(sample[0])\n",
    "    data[\"sepal width (cm)\"].append(sample[1])\n",
    "    data[\"petal length (cm)\"].append(sample[2])\n",
    "    data[\"petal width (cm)\"].append(sample[3])\n",
    "\n",
    "for cls in pred_classes:\n",
    "    data[\"Class\"].append(cls)\n",
    "\n",
    "for prob in pred_probability:\n",
    "    data[\"Probability\"].append(prob*100)\n",
    "\n",
    "generated_df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = generated_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict\n",
    "prediction_list = detector.predict(inputs).tolist()\n",
    "\n",
    "# Count anomalies\n",
    "anomalies = prediction_list.count(-1) / len(inputs)\n",
    "print(f\"Anomaly rate: {anomalies:.2%}\")\n",
    "\n",
    "# Convert inputs and predictions to NumPy arrays\n",
    "inputs = np.array(inputs)\n",
    "predictions = np.array(prediction_list)\n",
    "\n",
    "# Extract anomalous inputs\n",
    "anomalous_inputs = inputs[predictions == -1]\n",
    "\n",
    "\n",
    "\n",
    "# Keep only rows where prediction == -1 (anomaly)\n",
    "generated_df_anomalies = generated_df[predictions == -1].copy()\n",
    "\n",
    "# Ensure generated_df aligns with inputs\n",
    "generated_df_anomalies = generated_df_anomalies.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02781350",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df_anomalies=generated_df_anomalies.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221965b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = detector.predict(X).tolist()\n",
    "\n",
    "anomalies = prediction_list.count(-1)/len(X)\n",
    "\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea18666",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098a872",
   "metadata": {},
   "source": [
    "# Transferability Of Confidently Clasified Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f54310",
   "metadata": {},
   "source": [
    "## Testing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94641e46",
   "metadata": {},
   "source": [
    "## Testing on Randomn Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RF_model = RandomForestClassifier(n_estimators=200,random_state=42)\n",
    "RF_model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = RF_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred,average=\"macro\")\n",
    "test_recall = recall_score(y_test, y_pred,average=\"macro\")\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f48289",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_anomalies = generated_df_anomalies.drop(columns =[\"Class\",\"Probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for all inputs at once\n",
    "probs_all = RF_model.predict_proba(generated_anomalies.values)\n",
    "\n",
    "# Get max probabilities and predicted classes\n",
    "max_probs = np.max(probs_all, axis=1)\n",
    "pred_classes = np.argmax(probs_all, axis=1)\n",
    "\n",
    "# Filter by threshold\n",
    "mask = max_probs >= confidence_threshold\n",
    "RF_anomalies_list = generated_anomalies.values[mask]\n",
    "max_prob_rf = pred_classes[mask]\n",
    "high_confidence_count = np.sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness = high_confidence_count/len(anomalous_inputs)\n",
    "print(robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]\n",
    "RF_df = pd.DataFrame(RF_anomalies_list, columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RF_anomalies_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa99401",
   "metadata": {},
   "source": [
    "## Testing On Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Model architecture\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(20, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "test_model = SimpleNet(input_dim=X_train_tensor.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(test_model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    test_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = test_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        test_model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = test_model(X_test_tensor)\n",
    "            preds = torch.argmax(test_outputs, dim=1)\n",
    "            acc = accuracy_score(y_test_tensor, preds)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Test Accuracy: {acc * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_model.eval()\n",
    "correct = 0\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(X_test_scaled.shape[0]):\n",
    "    x_sample = torch.tensor(X_test_scaled[i].reshape(1, -1), dtype=torch.float32)\n",
    "    y_sample = y_test_encoded[i].reshape(-1)  # Assuming one-hot encoded\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = test_model(x_sample)\n",
    "        probs = torch.softmax(logits, dim=1).numpy().flatten()\n",
    "        pred_class = np.argmax(probs)\n",
    "        true_class = np.argmax(y_sample)\n",
    "\n",
    "    y_preds.append(pred_class)\n",
    "    y_true.append(true_class)\n",
    "\n",
    "    if pred_class == true_class:\n",
    "        correct += 1\n",
    "\n",
    "# Single sample prediction (optional)\n",
    "x_sample = torch.tensor(X_test_scaled[0].reshape(1, -1), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    pred = test_model(x_sample)\n",
    "\n",
    "# Metrics\n",
    "test_accuracy = correct / len(X_test_scaled)\n",
    "precision = precision_score(y_true, y_preds, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true, y_preds, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_true, y_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Precision:     {precision * 100:.2f}%\")\n",
    "print(f\"Recall:        {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score:      {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MLP_anomalies_list = []\n",
    "high_confidence_count = 0\n",
    "test = []\n",
    "# Scale inputs\n",
    "scaled_anomalous_inputs = ANN_scaler.transform(generated_anomalies.values)\n",
    "\n",
    "# Set model to eval mode\n",
    "test_model.eval()\n",
    "\n",
    "# Loop through each anomalous input\n",
    "for x in scaled_anomalous_inputs:\n",
    "    x_tensor = torch.tensor(x.reshape(1, -1), dtype=torch.float32)  # shape: [1, input_dim]\n",
    "    with torch.no_grad():\n",
    "        logits = test_model(x_tensor)  # shape: [1, 2]\n",
    "        probs = torch.softmax(logits, dim=1).numpy().flatten()  # convert to numpy array\n",
    "        max_prob = np.max(probs)\n",
    "\n",
    "    if max_prob >= confidence_threshold:\n",
    "        high_confidence_count += 1\n",
    "        MLP_anomalies_list.append(x)\n",
    "        if np.argmax(probs) ==1:\n",
    "            test.append(x)\n",
    "\n",
    "# Inverse transform to original feature space\n",
    "MLP_anomalies_list = ANN_scaler.inverse_transform(MLP_anomalies_list)\n",
    "\n",
    "# Compute robustness score\n",
    "robustness = high_confidence_count / len(generated_anomalies.values)\n",
    "print(f\"Robustness: {robustness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_anomalies.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MLP_anomalies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]\n",
    "MLP_df = pd.DataFrame(MLP_anomalies_list, columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bf258",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n = 5\n",
    "neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = neigh.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred,average=\"macro\")\n",
    "test_recall = recall_score(y_test, y_pred,average=\"macro\")\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_anomalies_list = []\n",
    "high_confidence_count = 0\n",
    "\n",
    "scaled_anomalous_inputs = ANN_scaler.transform(generated_anomalies.values)\n",
    "\n",
    "for x in scaled_anomalous_inputs:\n",
    "    probs = neigh.predict_proba(x.reshape(1, -1))  # Get class probabilities\n",
    "    max_prob = np.max(probs)                       # Highest class probability\n",
    "\n",
    "    if max_prob >= confidence_threshold:\n",
    "        high_confidence_count += 1\n",
    "        KNN_anomalies_list.append(x)\n",
    "\n",
    "robustness = high_confidence_count / len(scaled_anomalous_inputs)\n",
    "KNN_anomalies_list = ANN_scaler.inverse_transform(KNN_anomalies_list)\n",
    "print(robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(KNN_anomalies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0fd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]\n",
    "knn_df = pd.DataFrame(KNN_anomalies_list, columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b6696",
   "metadata": {},
   "source": [
    "## Checking Shared Vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop duplicates in each DataFrame\n",
    "df_rf = RF_df.drop_duplicates()\n",
    "df_knn = knn_df.drop_duplicates()\n",
    "df_nn = MLP_df.drop_duplicates()\n",
    "\n",
    "# Merge on all columns to find common rows\n",
    "common_rows = df_rf.merge(df_knn, how='inner').merge(df_nn, how='inner')\n",
    "\n",
    "print(\"Number of common rows:\", len(common_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare\n",
    "generated_df_copy = generated_df_anomalies.copy()\n",
    "generated_df_copy.drop_duplicates().reset_index(drop=True)\n",
    "cols = generated_df.columns[:4]  # First four columns\n",
    "\n",
    "test_df1 = pd.merge(generated_df_copy[cols],knn_df,how=\"outer\",on=feature_names,indicator=True)\n",
    "test_df2 = pd.merge(generated_df_copy[cols],RF_df,how=\"outer\",on=feature_names,indicator=True)\n",
    "test_df3 = pd.merge(generated_df_copy[cols],MLP_df,how=\"outer\",on=feature_names,indicator=True)\n",
    "\n",
    "generated_df_copy['knn_overlap'] = test_df1['_merge']\n",
    "generated_df_copy['RF_overlap'] = test_df2['_merge']\n",
    "generated_df_copy['MLP_overlap'] = test_df3['_merge']\n",
    "\n",
    "overlap_map = {\"both\": True, \"left_only\": False}\n",
    "generated_df_copy[\"knn_overlap\"] = generated_df_copy[\"knn_overlap\"].map(overlap_map)\n",
    "generated_df_copy['RF_overlap'] = generated_df_copy[\"RF_overlap\"].map(overlap_map)\n",
    "generated_df_copy['MLP_overlap'] = generated_df_copy[\"MLP_overlap\"].map(overlap_map)\n",
    "\n",
    "generated_df_copy[\"all_overlap\"] = (\n",
    "    (generated_df_copy[\"knn_overlap\"] == True) &\n",
    "    (generated_df_copy[\"RF_overlap\"] == True) &\n",
    "    (generated_df_copy[\"MLP_overlap\"] == True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df_copy[\"all_overlap\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb66d4f",
   "metadata": {},
   "source": [
    "## Decision Boundaries - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c15731",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]\n",
    "\n",
    "# Create DataFrame with original (unscaled) values\n",
    "iris_df = pd.DataFrame(X, columns=feature_names)\n",
    "iris_df[\"Class\"] = y  # Numeric class labels (0, 1, 2)\n",
    "iris_df[\"Source\"] = \"Original\"  # Mark as original data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_decision_boundary_KNN(classifier, original_data, generated_data,features,model_name,scaler = \n",
    "                                   ANN_scaler,resolution=0.02):\n",
    "    pca_scaler = StandardScaler()\n",
    "    original_scaled = pca_scaler.fit_transform(original_data[features])\n",
    "    generated_scaled = pca_scaler.transform(generated_data[features])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    original_pca = pca.fit_transform(original_scaled)\n",
    "    generated_pca = pca.transform(generated_scaled)\n",
    "\n",
    "    all_pca = np.vstack([original_pca, generated_pca])\n",
    "    x_min, x_max = all_pca[:, 0].min() - 1, all_pca[:, 0].max() + 1\n",
    "    y_min, y_max = all_pca[:, 1].min() - 1, all_pca[:, 1].max() + 1 \n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    \n",
    "\n",
    "    X_mesh_pca = np.c_[xx.ravel(), yy.ravel()]\n",
    "    X_mesh_input_scaled = pca.inverse_transform(X_mesh_pca)\n",
    "    X_mesh_input_unscaled = pca_scaler.inverse_transform(X_mesh_input_scaled)\n",
    "\n",
    "    sample = scaler.transform(X_mesh_input_unscaled)\n",
    "    \n",
    "\n",
    "    zz = (classifier.predict(sample)) \n",
    "    zz = zz.reshape(xx.shape)\n",
    "\n",
    "    \n",
    "    #print(\"Unique predictions on mesh grid:\", np.unique(zz))\n",
    "    generated_df_copy.loc[generated_df_copy[\"knn_overlap\"] == True, \"Source\"] = \"Generated (Above Threshold)\"\n",
    "    generated_df_copy.loc[generated_df_copy[\"knn_overlap\"] != True, \"Source\"] = \"Generated (Below Threshold)\"\n",
    "\n",
    "    cmap = ListedColormap([\"#e41a1c\", \"#377eb8\", \"#4daf4a\"])  # Setosa, Versicolour, Virginica\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, zz, levels=np.arange(-0.5, 3.5, 1), cmap=cmap, alpha=0.3)\n",
    "\n",
    "    class_map = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"}\n",
    "    original_data[\"ClassName\"] = original_data[\"Class\"].map(class_map)\n",
    "    generated_data[\"ClassName\"] = generated_data[\"Class\"].map(class_map)\n",
    "\n",
    "    original_data[\"PCA1\"] = original_pca[:, 0]\n",
    "    original_data[\"PCA2\"] = original_pca[:, 1]\n",
    "    generated_data[\"PCA1\"] = generated_pca[:, 0]\n",
    "    generated_data[\"PCA2\"] = generated_pca[:, 1]\n",
    "\n",
    "    palette = {\n",
    "        \"Setosa\": \"#e41a1c\",\n",
    "        \"Versicolor\": \"#377eb8\",\n",
    "        \"Virginica\": \"#4daf4a\"\n",
    "    }\n",
    "\n",
    "    combined_data = pd.concat([original_data, generated_data], ignore_index=True)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=original_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"ClassName\",                   \n",
    "        palette=palette,\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    palette = {\n",
    "        \"Generated (Above Threshold)\": \"#e4931a\",\n",
    "        \"Generated (Below Threshold)\": \"#9d9e9f\",\n",
    "    }\n",
    "    sns.scatterplot(\n",
    "        data=generated_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"Source\", \n",
    "        style=\"Source\",                  \n",
    "        palette= palette,\n",
    "        markers= {\"Generated (Above Threshold)\" : \"X\",\"Generated (Below Threshold)\" : 'D'},\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.legend(title=\"Class/Generated Confidence\", bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=\"small\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(f\"Decision Boundaries in PCA Space with Original and Generated Data ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b893cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_decision_boundary_ANN(classifier, original_data, generated_data, \n",
    "                                   features,ANN_scaler, model_name, resolution=0.02, device='cpu'):\n",
    "\n",
    "    # Scale and apply PCA\n",
    "    pca_scaler = StandardScaler()\n",
    "    original_scaled = pca_scaler.fit_transform(original_data[features])\n",
    "    generated_scaled = pca_scaler.transform(generated_data[features])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    original_pca = pca.fit_transform(original_scaled)\n",
    "    generated_pca = pca.transform(generated_scaled)\n",
    "\n",
    "    # Mesh grid\n",
    "    all_pca = np.vstack([original_pca, generated_pca])\n",
    "    x_min, x_max = all_pca[:, 0].min() - 1, all_pca[:, 0].max() + 1\n",
    "    y_min, y_max = all_pca[:, 1].min() - 1, all_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "\n",
    "    X_mesh_pca = np.c_[xx.ravel(), yy.ravel()]\n",
    "    X_mesh_input = pca.inverse_transform(X_mesh_pca)\n",
    "    X_mesh_input_unscaled = pca_scaler.inverse_transform(X_mesh_input)\n",
    "    sample = ANN_scaler.transform(X_mesh_input_unscaled)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    sample_tensor = torch.tensor(sample, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Inference\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(sample_tensor)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    zz = preds.reshape(xx.shape)\n",
    "\n",
    "    # Annotate generated data\n",
    "    generated_data.loc[generated_data[\"MLP_overlap\"] == True, \"Source\"] = \"Generated (Above Threshold)\"\n",
    "    generated_data.loc[generated_data[\"MLP_overlap\"] != True, \"Source\"] = \"Generated (Below Threshold)\"\n",
    "\n",
    "    # Plot decision boundary\n",
    "    cmap = ListedColormap([\"#e41a1c\", \"#377eb8\", \"#4daf4a\"])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, zz, levels=np.arange(-0.5, 3.5, 1), cmap=cmap, alpha=0.3)\n",
    "\n",
    "    # Map class labels\n",
    "    class_map = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"}\n",
    "    original_data[\"ClassName\"] = original_data[\"Class\"].map(class_map)\n",
    "    generated_data[\"ClassName\"] = generated_data[\"Class\"].map(class_map)\n",
    "\n",
    "    original_data[\"PCA1\"] = original_pca[:, 0]\n",
    "    original_data[\"PCA2\"] = original_pca[:, 1]\n",
    "    generated_data[\"PCA1\"] = generated_pca[:, 0]\n",
    "    generated_data[\"PCA2\"] = generated_pca[:, 1]\n",
    "\n",
    "    # Plot original data\n",
    "    palette = {\n",
    "        \"Setosa\": \"#e41a1c\",\n",
    "        \"Versicolor\": \"#377eb8\",\n",
    "        \"Virginica\": \"#4daf4a\"\n",
    "    }\n",
    "    sns.scatterplot(\n",
    "        data=original_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"ClassName\",\n",
    "        palette=palette,\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Plot generated data\n",
    "    palette = {\n",
    "        \"Generated (Above Threshold)\": \"#e4931a\",\n",
    "        \"Generated (Below Threshold)\": \"#9d9e9f\",\n",
    "    }\n",
    "    sns.scatterplot(\n",
    "        data=generated_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"Source\",\n",
    "        style=\"Source\",\n",
    "        palette=palette,\n",
    "        markers={\"Generated (Above Threshold)\": \"X\", \"Generated (Below Threshold)\": 'D'},\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    plt.legend(title=\"Class/Generated Confidence\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=\"small\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(f\"Decision Boundaries with Original and Generated Data ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_decision_boundary_RF(classifier, original_data, generated_data,features,model_name\n",
    "                                  ,resolution=0.02):\n",
    "    pca_scaler = StandardScaler()\n",
    "    original_scaled = pca_scaler.fit_transform(original_data[features])\n",
    "    generated_scaled = pca_scaler.transform(generated_data[features])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    original_pca = pca.fit_transform(original_scaled)\n",
    "    generated_pca = pca.transform(generated_scaled)\n",
    "\n",
    "    all_pca = np.vstack([original_pca, generated_pca])\n",
    "    x_min, x_max = all_pca[:, 0].min() - 1, all_pca[:, 0].max() + 1\n",
    "    y_min, y_max = all_pca[:, 1].min() - 1, all_pca[:, 1].max() + 1 \n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    \n",
    "\n",
    "    X_mesh_pca = np.c_[xx.ravel(), yy.ravel()]\n",
    "    X_mesh_input_scaled = pca.inverse_transform(X_mesh_pca)\n",
    "    X_mesh_input_unscaled = pca_scaler.inverse_transform(X_mesh_input_scaled)\n",
    "\n",
    "    zz = classifier.predict(X_mesh_input_unscaled)\n",
    "    zz = zz.reshape(xx.shape)\n",
    "    \n",
    "    #print(\"Unique predictions on mesh grid:\", np.unique(zz))\n",
    "    generated_df_copy.loc[generated_df_copy[\"RF_overlap\"] == True, \"Source\"] = \"Generated (Above Threshold)\"\n",
    "    generated_df_copy.loc[generated_df_copy[\"RF_overlap\"] != True, \"Source\"] = \"Generated (Below Threshold)\"\n",
    "\n",
    "    cmap = ListedColormap([\"#e41a1c\", \"#377eb8\", \"#4daf4a\"])  # Setosa, Versicolour, Virginica\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, zz, levels=np.arange(-0.5, 3.5, 1), cmap=cmap, alpha=0.3)\n",
    "\n",
    "    class_map = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"}\n",
    "    original_data[\"ClassName\"] = original_data[\"Class\"].map(class_map)\n",
    "    generated_data[\"ClassName\"] = generated_data[\"Class\"].map(class_map)\n",
    "\n",
    "    original_data[\"PCA1\"] = original_pca[:, 0]\n",
    "    original_data[\"PCA2\"] = original_pca[:, 1]\n",
    "    generated_data[\"PCA1\"] = generated_pca[:, 0]\n",
    "    generated_data[\"PCA2\"] = generated_pca[:, 1]\n",
    "\n",
    "    palette = {\n",
    "        \"Setosa\": \"#e41a1c\",\n",
    "        \"Versicolor\": \"#377eb8\",\n",
    "        \"Virginica\": \"#4daf4a\"\n",
    "    }\n",
    "\n",
    "    combined_data = pd.concat([original_data, generated_data], ignore_index=True)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=original_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"ClassName\",                   \n",
    "        palette=palette,\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    palette = {\n",
    "        \"Generated (Above Threshold)\": \"#e4931a\",\n",
    "        \"Generated (Below Threshold)\": \"#9d9e9f\",\n",
    "    }\n",
    "    sns.scatterplot(\n",
    "        data=generated_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"Source\", \n",
    "        style=\"Source\",                  \n",
    "        palette= palette,\n",
    "        markers= {\"Generated (Above Threshold)\" : \"X\",\"Generated (Below Threshold)\" : 'D'},\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.legend(title=\"Class/Generated Confidence\", bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=\"small\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(f\"Decision Boundaries in PCA Space with Original and Generated Data ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ba803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_decision_boundary_Invertible(classifier, original_data, generated_data, features,model_name, \n",
    "                                   resolution=0.02):\n",
    "\n",
    "    pca_scaler = StandardScaler()\n",
    "    original_scaled = pca_scaler.fit_transform(original_data[features])\n",
    "    generated_scaled = pca_scaler.transform(generated_data[features])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    original_pca = pca.fit_transform(original_scaled)\n",
    "    generated_pca = pca.transform(generated_scaled)\n",
    "\n",
    "    all_pca = np.vstack([original_pca, generated_pca])\n",
    "    x_min, x_max = all_pca[:, 0].min() - 1, all_pca[:, 0].max() + 1\n",
    "    y_min, y_max = all_pca[:, 1].min() - 1, all_pca[:, 1].max() + 1 \n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    \n",
    "\n",
    "    X_mesh_pca = np.c_[xx.ravel(), yy.ravel()]\n",
    "    X_mesh_input = pca.inverse_transform(X_mesh_pca)\n",
    "    X_mesh_input_unscaled = pca_scaler.inverse_transform(X_mesh_input)\n",
    "    sample = ANN_scaler.transform(X_mesh_input_unscaled)\n",
    "\n",
    "    zz = np.array([\n",
    "        np.argmax(classifier.forward(x.reshape(-1, 1))[:3]) for x in sample\n",
    "    ])\n",
    "\n",
    "    zz = zz.reshape(xx.shape)\n",
    "    \n",
    "    #print(\"Unique predictions on mesh grid:\", np.unique(zz))\n",
    "    generated_df_copy.loc[generated_df_copy[\"MLP_overlap\"] == True, \"Source\"] = \"Generated (Above Threshold)\"\n",
    "    generated_df_copy.loc[generated_df_copy[\"MLP_overlap\"] != True, \"Source\"] = \"Generated (Below Threshold)\"\n",
    "\n",
    "    cmap = ListedColormap([\"#e41a1c\", \"#377eb8\", \"#4daf4a\"])  # Setosa, Versicolour, Virginica\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, zz, levels=np.arange(-0.5, 3.5, 1), cmap=cmap, alpha=0.3)\n",
    "\n",
    "    class_map = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"}\n",
    "    original_data[\"ClassName\"] = original_data[\"Class\"].map(class_map)\n",
    "    generated_data[\"ClassName\"] = generated_data[\"Class\"].map(class_map)\n",
    "\n",
    "    original_data[\"PCA1\"] = original_pca[:, 0]\n",
    "    original_data[\"PCA2\"] = original_pca[:, 1]\n",
    "    generated_data[\"PCA1\"] = generated_pca[:, 0]\n",
    "    generated_data[\"PCA2\"] = generated_pca[:, 1]\n",
    "\n",
    "    palette = {\n",
    "        \"Setosa\": \"#e41a1c\",\n",
    "        \"Versicolor\": \"#377eb8\",\n",
    "        \"Virginica\": \"#4daf4a\"\n",
    "    }\n",
    "\n",
    "    combined_data = pd.concat([original_data, generated_data], ignore_index=True)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=original_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",\n",
    "        hue=\"ClassName\",                   \n",
    "        palette=palette,\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=generated_data,\n",
    "        x=\"PCA1\",\n",
    "        y=\"PCA2\",                 \n",
    "        color=  \"#e4931a\",\n",
    "        marker=  \"X\",\n",
    "        s=80,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.7,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.legend(title=\"Class/Generated Confidence\", bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=\"small\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(f\"Decision Boundaries in PCA Space with Original and Generated Data ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_decision_boundary_KNN(neigh, iris_df, generated_df_copy,feature_names,model_name= \"K-NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_decision_boundary_RF(RF_model, iris_df, generated_df_copy,feature_names,model_name = \"Randomn Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_decision_boundary_Invertible(model, iris_df, generated_df_copy,feature_names,model_name = \"Invertible MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_decision_boundary_ANN(test_model, iris_df, generated_df_copy,feature_names,\n",
    "                               ANN_scaler,model_name = \"MLP (1 layer, 20 neuron)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30dbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = iris_df.copy()\n",
    "generated_data = generated_df_copy.copy()\n",
    "\n",
    "pca_scaler = StandardScaler()\n",
    "original_scaled = pca_scaler.fit_transform(original_data[feature_names])\n",
    "generated_scaled = pca_scaler.transform(generated_data[feature_names])\n",
    "\n",
    "# Fit PCA only on scaled original data\n",
    "pca = PCA(n_components=2) # Only use two principal components\n",
    "pca.fit(original_scaled)\n",
    "\n",
    "# Project both datasets using same PCA\n",
    "original_pca = pca.transform(original_scaled)\n",
    "generated_pca = pca.transform(generated_scaled)\n",
    "\n",
    "# Add PCA results to DataFrames\n",
    "original_data[\"PCA1\"] = original_pca[:, 0]\n",
    "original_data[\"PCA2\"] = original_pca[:, 1]\n",
    "generated_data[\"PCA1\"] = generated_pca[:, 0]\n",
    "generated_data[\"PCA2\"] = generated_pca[:, 1]\n",
    "generated_data.loc[generated_df_copy[\"all_overlap\"] == True, \"Source\"] = \"Generated Shared\"\n",
    "generated_data.loc[generated_df_copy[\"all_overlap\"] != True, \"Source\"] = \"Generated Not Shared\"\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "palette = {\n",
    "    \"Setosa\": \"#e41a1c\",\n",
    "    \"Versicolor\": \"#377eb8\",\n",
    "    \"Virginica\": \"#4daf4a\"\n",
    "}\n",
    "combined_data = pd.concat([original_data, generated_data], ignore_index=True)\n",
    "# Create the plot\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=original_data,\n",
    "    x=\"PCA1\",\n",
    "    y=\"PCA2\",\n",
    "    hue=\"ClassName\",                   \n",
    "    palette=palette,\n",
    "    s=80,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.7,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "\n",
    "palette = {\n",
    "    \"Generated Shared\": \"#e4931a\",\n",
    "    \"Generated Not Shared\": \"#9d9e9f\",\n",
    "}\n",
    "sns.scatterplot(\n",
    "    data=generated_data,\n",
    "    x=\"PCA1\",\n",
    "    y=\"PCA2\",\n",
    "    hue=\"Source\", \n",
    "    style=\"Source\",                  \n",
    "    palette= palette,\n",
    "    markers= {\"Generated Shared\" : \"X\",\"Generated Not Shared\" : 'D'},\n",
    "    s=80,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.7,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.legend(title=\"Class / Source\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title(\"PCA Projection on Scaled Features (Fitted on Original Data Only)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
